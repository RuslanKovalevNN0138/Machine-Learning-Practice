{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ekvu3v7C5A6b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OQXLQaPBNK2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.dp1 = nn.Dropout2d(0.10)\n",
        "        self.dp2 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.cn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dp1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dp2(x)\n",
        "        x = self.fc2(x)\n",
        "        op = F.log_softmax(x, dim=1)\n",
        "        return op"
      ],
      "metadata": {
        "id": "cHNhQ4NSNR6n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "yy3iZCEkNiDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_dataloader, optim, epoch):\n",
        "    model.train()\n",
        "    for b_i, (X, y) in enumerate(train_dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optim.zero_grad()\n",
        "        pred_prob = model(X)\n",
        "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if b_i % 10 == 0:\n",
        "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
        "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
        "                100. * b_i / len(train_dataloader), loss.item()))"
      ],
      "metadata": {
        "id": "CZBZEGKlNZJw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_dataloader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    success = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred_prob = model(X)\n",
        "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
        "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
        "            success += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    loss /= len(test_dataloader.dataset)\n",
        "\n",
        "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        loss, success, len(test_dataloader.dataset),\n",
        "        100. * success / len(test_dataloader.dataset)))"
      ],
      "metadata": {
        "id": "qLeFQEsWNluR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ],
      "metadata": {
        "id": "4Exg0vPZNpst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
        "    batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))\n",
        "                   ])),\n",
        "    batch_size=500, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb0WKBz7Ntkz",
        "outputId": "a262309c-8575-4407-ed28-dd10e68c4fd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 37.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.24MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.6MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.92MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training epochs"
      ],
      "metadata": {
        "id": "7bNAri8BN0fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = ConvNet()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "jkqxQ90HN4bX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_dataloader, optimizer, epoch)\n",
        "    test(model, device, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfSs4ZujN69S",
        "outputId": "69638c5d-4a55-42ca-fc82-b0311c420668"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
            "epoch: 1 [320/60000 (1%)]\t training loss: 1.924133\n",
            "epoch: 1 [640/60000 (1%)]\t training loss: 1.313336\n",
            "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
            "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
            "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678430\n",
            "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477187\n",
            "epoch: 1 [2240/60000 (4%)]\t training loss: 0.529062\n",
            "epoch: 1 [2560/60000 (4%)]\t training loss: 0.468829\n",
            "epoch: 1 [2880/60000 (5%)]\t training loss: 0.242887\n",
            "epoch: 1 [3200/60000 (5%)]\t training loss: 0.519633\n",
            "epoch: 1 [3520/60000 (6%)]\t training loss: 0.262872\n",
            "epoch: 1 [3840/60000 (6%)]\t training loss: 0.467795\n",
            "epoch: 1 [4160/60000 (7%)]\t training loss: 0.417298\n",
            "epoch: 1 [4480/60000 (7%)]\t training loss: 0.316030\n",
            "epoch: 1 [4800/60000 (8%)]\t training loss: 0.498437\n",
            "epoch: 1 [5120/60000 (9%)]\t training loss: 0.157880\n",
            "epoch: 1 [5440/60000 (9%)]\t training loss: 0.364403\n",
            "epoch: 1 [5760/60000 (10%)]\t training loss: 0.085984\n",
            "epoch: 1 [6080/60000 (10%)]\t training loss: 0.177615\n",
            "epoch: 1 [6400/60000 (11%)]\t training loss: 0.278263\n",
            "epoch: 1 [6720/60000 (11%)]\t training loss: 0.082559\n",
            "epoch: 1 [7040/60000 (12%)]\t training loss: 0.351368\n",
            "epoch: 1 [7360/60000 (12%)]\t training loss: 0.040539\n",
            "epoch: 1 [7680/60000 (13%)]\t training loss: 0.367039\n",
            "epoch: 1 [8000/60000 (13%)]\t training loss: 0.219277\n",
            "epoch: 1 [8320/60000 (14%)]\t training loss: 0.329156\n",
            "epoch: 1 [8640/60000 (14%)]\t training loss: 0.214590\n",
            "epoch: 1 [8960/60000 (15%)]\t training loss: 0.057667\n",
            "epoch: 1 [9280/60000 (15%)]\t training loss: 0.345586\n",
            "epoch: 1 [9600/60000 (16%)]\t training loss: 0.205804\n",
            "epoch: 1 [9920/60000 (17%)]\t training loss: 0.209767\n",
            "epoch: 1 [10240/60000 (17%)]\t training loss: 0.209993\n",
            "epoch: 1 [10560/60000 (18%)]\t training loss: 0.403128\n",
            "epoch: 1 [10880/60000 (18%)]\t training loss: 0.071791\n",
            "epoch: 1 [11200/60000 (19%)]\t training loss: 0.069998\n",
            "epoch: 1 [11520/60000 (19%)]\t training loss: 0.231422\n",
            "epoch: 1 [11840/60000 (20%)]\t training loss: 0.072475\n",
            "epoch: 1 [12160/60000 (20%)]\t training loss: 0.409647\n",
            "epoch: 1 [12480/60000 (21%)]\t training loss: 0.021981\n",
            "epoch: 1 [12800/60000 (21%)]\t training loss: 0.093394\n",
            "epoch: 1 [13120/60000 (22%)]\t training loss: 0.085837\n",
            "epoch: 1 [13440/60000 (22%)]\t training loss: 0.161934\n",
            "epoch: 1 [13760/60000 (23%)]\t training loss: 0.065058\n",
            "epoch: 1 [14080/60000 (23%)]\t training loss: 0.046802\n",
            "epoch: 1 [14400/60000 (24%)]\t training loss: 0.083767\n",
            "epoch: 1 [14720/60000 (25%)]\t training loss: 0.167542\n",
            "epoch: 1 [15040/60000 (25%)]\t training loss: 0.345123\n",
            "epoch: 1 [15360/60000 (26%)]\t training loss: 0.091196\n",
            "epoch: 1 [15680/60000 (26%)]\t training loss: 0.108777\n",
            "epoch: 1 [16000/60000 (27%)]\t training loss: 0.075832\n",
            "epoch: 1 [16320/60000 (27%)]\t training loss: 0.171944\n",
            "epoch: 1 [16640/60000 (28%)]\t training loss: 0.029883\n",
            "epoch: 1 [16960/60000 (28%)]\t training loss: 0.333812\n",
            "epoch: 1 [17280/60000 (29%)]\t training loss: 0.060629\n",
            "epoch: 1 [17600/60000 (29%)]\t training loss: 0.028994\n",
            "epoch: 1 [17920/60000 (30%)]\t training loss: 0.040753\n",
            "epoch: 1 [18240/60000 (30%)]\t training loss: 0.019892\n",
            "epoch: 1 [18560/60000 (31%)]\t training loss: 0.057777\n",
            "epoch: 1 [18880/60000 (31%)]\t training loss: 0.236884\n",
            "epoch: 1 [19200/60000 (32%)]\t training loss: 0.088455\n",
            "epoch: 1 [19520/60000 (33%)]\t training loss: 0.067441\n",
            "epoch: 1 [19840/60000 (33%)]\t training loss: 0.063453\n",
            "epoch: 1 [20160/60000 (34%)]\t training loss: 0.107841\n",
            "epoch: 1 [20480/60000 (34%)]\t training loss: 0.060509\n",
            "epoch: 1 [20800/60000 (35%)]\t training loss: 0.205607\n",
            "epoch: 1 [21120/60000 (35%)]\t training loss: 0.280535\n",
            "epoch: 1 [21440/60000 (36%)]\t training loss: 0.262102\n",
            "epoch: 1 [21760/60000 (36%)]\t training loss: 0.109629\n",
            "epoch: 1 [22080/60000 (37%)]\t training loss: 0.096564\n",
            "epoch: 1 [22400/60000 (37%)]\t training loss: 0.134667\n",
            "epoch: 1 [22720/60000 (38%)]\t training loss: 0.265525\n",
            "epoch: 1 [23040/60000 (38%)]\t training loss: 0.076615\n",
            "epoch: 1 [23360/60000 (39%)]\t training loss: 0.125220\n",
            "epoch: 1 [23680/60000 (39%)]\t training loss: 0.251269\n",
            "epoch: 1 [24000/60000 (40%)]\t training loss: 0.215296\n",
            "epoch: 1 [24320/60000 (41%)]\t training loss: 0.065185\n",
            "epoch: 1 [24640/60000 (41%)]\t training loss: 0.140391\n",
            "epoch: 1 [24960/60000 (42%)]\t training loss: 0.027044\n",
            "epoch: 1 [25280/60000 (42%)]\t training loss: 0.024160\n",
            "epoch: 1 [25600/60000 (43%)]\t training loss: 0.570725\n",
            "epoch: 1 [25920/60000 (43%)]\t training loss: 0.460728\n",
            "epoch: 1 [26240/60000 (44%)]\t training loss: 0.129823\n",
            "epoch: 1 [26560/60000 (44%)]\t training loss: 0.040875\n",
            "epoch: 1 [26880/60000 (45%)]\t training loss: 0.150539\n",
            "epoch: 1 [27200/60000 (45%)]\t training loss: 0.018501\n",
            "epoch: 1 [27520/60000 (46%)]\t training loss: 0.040860\n",
            "epoch: 1 [27840/60000 (46%)]\t training loss: 0.136634\n",
            "epoch: 1 [28160/60000 (47%)]\t training loss: 0.048300\n",
            "epoch: 1 [28480/60000 (47%)]\t training loss: 0.392610\n",
            "epoch: 1 [28800/60000 (48%)]\t training loss: 0.119276\n",
            "epoch: 1 [29120/60000 (49%)]\t training loss: 0.198008\n",
            "epoch: 1 [29440/60000 (49%)]\t training loss: 0.014311\n",
            "epoch: 1 [29760/60000 (50%)]\t training loss: 0.053688\n",
            "epoch: 1 [30080/60000 (50%)]\t training loss: 0.541627\n",
            "epoch: 1 [30400/60000 (51%)]\t training loss: 0.127081\n",
            "epoch: 1 [30720/60000 (51%)]\t training loss: 0.230976\n",
            "epoch: 1 [31040/60000 (52%)]\t training loss: 0.080813\n",
            "epoch: 1 [31360/60000 (52%)]\t training loss: 0.070519\n",
            "epoch: 1 [31680/60000 (53%)]\t training loss: 0.026341\n",
            "epoch: 1 [32000/60000 (53%)]\t training loss: 0.011774\n",
            "epoch: 1 [32320/60000 (54%)]\t training loss: 0.110159\n",
            "epoch: 1 [32640/60000 (54%)]\t training loss: 0.095929\n",
            "epoch: 1 [32960/60000 (55%)]\t training loss: 0.042572\n",
            "epoch: 1 [33280/60000 (55%)]\t training loss: 0.260714\n",
            "epoch: 1 [33600/60000 (56%)]\t training loss: 0.192777\n",
            "epoch: 1 [33920/60000 (57%)]\t training loss: 0.019657\n",
            "epoch: 1 [34240/60000 (57%)]\t training loss: 0.024119\n",
            "epoch: 1 [34560/60000 (58%)]\t training loss: 0.002996\n",
            "epoch: 1 [34880/60000 (58%)]\t training loss: 0.175705\n",
            "epoch: 1 [35200/60000 (59%)]\t training loss: 0.279498\n",
            "epoch: 1 [35520/60000 (59%)]\t training loss: 0.133032\n",
            "epoch: 1 [35840/60000 (60%)]\t training loss: 0.012259\n",
            "epoch: 1 [36160/60000 (60%)]\t training loss: 0.141879\n",
            "epoch: 1 [36480/60000 (61%)]\t training loss: 0.105978\n",
            "epoch: 1 [36800/60000 (61%)]\t training loss: 0.136131\n",
            "epoch: 1 [37120/60000 (62%)]\t training loss: 0.046783\n",
            "epoch: 1 [37440/60000 (62%)]\t training loss: 0.229350\n",
            "epoch: 1 [37760/60000 (63%)]\t training loss: 0.023725\n",
            "epoch: 1 [38080/60000 (63%)]\t training loss: 0.125504\n",
            "epoch: 1 [38400/60000 (64%)]\t training loss: 0.044750\n",
            "epoch: 1 [38720/60000 (65%)]\t training loss: 0.422863\n",
            "epoch: 1 [39040/60000 (65%)]\t training loss: 0.047209\n",
            "epoch: 1 [39360/60000 (66%)]\t training loss: 0.201553\n",
            "epoch: 1 [39680/60000 (66%)]\t training loss: 0.020029\n",
            "epoch: 1 [40000/60000 (67%)]\t training loss: 0.153952\n",
            "epoch: 1 [40320/60000 (67%)]\t training loss: 0.078929\n",
            "epoch: 1 [40640/60000 (68%)]\t training loss: 0.169443\n",
            "epoch: 1 [40960/60000 (68%)]\t training loss: 0.198745\n",
            "epoch: 1 [41280/60000 (69%)]\t training loss: 0.204217\n",
            "epoch: 1 [41600/60000 (69%)]\t training loss: 0.136150\n",
            "epoch: 1 [41920/60000 (70%)]\t training loss: 0.039529\n",
            "epoch: 1 [42240/60000 (70%)]\t training loss: 0.011084\n",
            "epoch: 1 [42560/60000 (71%)]\t training loss: 0.041894\n",
            "epoch: 1 [42880/60000 (71%)]\t training loss: 0.121699\n",
            "epoch: 1 [43200/60000 (72%)]\t training loss: 0.057140\n",
            "epoch: 1 [43520/60000 (73%)]\t training loss: 0.245478\n",
            "epoch: 1 [43840/60000 (73%)]\t training loss: 0.057972\n",
            "epoch: 1 [44160/60000 (74%)]\t training loss: 0.153300\n",
            "epoch: 1 [44480/60000 (74%)]\t training loss: 0.068017\n",
            "epoch: 1 [44800/60000 (75%)]\t training loss: 0.214221\n",
            "epoch: 1 [45120/60000 (75%)]\t training loss: 0.019666\n",
            "epoch: 1 [45440/60000 (76%)]\t training loss: 0.183320\n",
            "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004745\n",
            "epoch: 1 [46080/60000 (77%)]\t training loss: 0.230460\n",
            "epoch: 1 [46400/60000 (77%)]\t training loss: 0.047609\n",
            "epoch: 1 [46720/60000 (78%)]\t training loss: 0.104887\n",
            "epoch: 1 [47040/60000 (78%)]\t training loss: 0.056543\n",
            "epoch: 1 [47360/60000 (79%)]\t training loss: 0.026343\n",
            "epoch: 1 [47680/60000 (79%)]\t training loss: 0.208909\n",
            "epoch: 1 [48000/60000 (80%)]\t training loss: 0.173453\n",
            "epoch: 1 [48320/60000 (81%)]\t training loss: 0.171665\n",
            "epoch: 1 [48640/60000 (81%)]\t training loss: 0.011326\n",
            "epoch: 1 [48960/60000 (82%)]\t training loss: 0.078636\n",
            "epoch: 1 [49280/60000 (82%)]\t training loss: 0.175529\n",
            "epoch: 1 [49600/60000 (83%)]\t training loss: 0.225299\n",
            "epoch: 1 [49920/60000 (83%)]\t training loss: 0.166357\n",
            "epoch: 1 [50240/60000 (84%)]\t training loss: 0.066043\n",
            "epoch: 1 [50560/60000 (84%)]\t training loss: 0.003922\n",
            "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004356\n",
            "epoch: 1 [51200/60000 (85%)]\t training loss: 0.197849\n",
            "epoch: 1 [51520/60000 (86%)]\t training loss: 0.025213\n",
            "epoch: 1 [51840/60000 (86%)]\t training loss: 0.018620\n",
            "epoch: 1 [52160/60000 (87%)]\t training loss: 0.012329\n",
            "epoch: 1 [52480/60000 (87%)]\t training loss: 0.009802\n",
            "epoch: 1 [52800/60000 (88%)]\t training loss: 0.033116\n",
            "epoch: 1 [53120/60000 (89%)]\t training loss: 0.028992\n",
            "epoch: 1 [53440/60000 (89%)]\t training loss: 0.013409\n",
            "epoch: 1 [53760/60000 (90%)]\t training loss: 0.037544\n",
            "epoch: 1 [54080/60000 (90%)]\t training loss: 0.011171\n",
            "epoch: 1 [54400/60000 (91%)]\t training loss: 0.117641\n",
            "epoch: 1 [54720/60000 (91%)]\t training loss: 0.013717\n",
            "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001378\n",
            "epoch: 1 [55360/60000 (92%)]\t training loss: 0.045877\n",
            "epoch: 1 [55680/60000 (93%)]\t training loss: 0.052783\n",
            "epoch: 1 [56000/60000 (93%)]\t training loss: 0.048510\n",
            "epoch: 1 [56320/60000 (94%)]\t training loss: 0.142568\n",
            "epoch: 1 [56640/60000 (94%)]\t training loss: 0.053807\n",
            "epoch: 1 [56960/60000 (95%)]\t training loss: 0.029687\n",
            "epoch: 1 [57280/60000 (95%)]\t training loss: 0.087261\n",
            "epoch: 1 [57600/60000 (96%)]\t training loss: 0.003207\n",
            "epoch: 1 [57920/60000 (97%)]\t training loss: 0.095157\n",
            "epoch: 1 [58240/60000 (97%)]\t training loss: 0.015331\n",
            "epoch: 1 [58560/60000 (98%)]\t training loss: 0.006808\n",
            "epoch: 1 [58880/60000 (98%)]\t training loss: 0.091058\n",
            "epoch: 1 [59200/60000 (99%)]\t training loss: 0.005085\n",
            "epoch: 1 [59520/60000 (99%)]\t training loss: 0.031503\n",
            "epoch: 1 [59840/60000 (100%)]\t training loss: 0.035041\n",
            "\n",
            "Test dataset: Overall Loss: 0.0481, Overall Accuracy: 9839/10000 (98%)\n",
            "\n",
            "epoch: 2 [0/60000 (0%)]\t training loss: 0.075432\n",
            "epoch: 2 [320/60000 (1%)]\t training loss: 0.046304\n",
            "epoch: 2 [640/60000 (1%)]\t training loss: 0.123041\n",
            "epoch: 2 [960/60000 (2%)]\t training loss: 0.068879\n",
            "epoch: 2 [1280/60000 (2%)]\t training loss: 0.054027\n",
            "epoch: 2 [1600/60000 (3%)]\t training loss: 0.002742\n",
            "epoch: 2 [1920/60000 (3%)]\t training loss: 0.076803\n",
            "epoch: 2 [2240/60000 (4%)]\t training loss: 0.025736\n",
            "epoch: 2 [2560/60000 (4%)]\t training loss: 0.112260\n",
            "epoch: 2 [2880/60000 (5%)]\t training loss: 0.008886\n",
            "epoch: 2 [3200/60000 (5%)]\t training loss: 0.077838\n",
            "epoch: 2 [3520/60000 (6%)]\t training loss: 0.015863\n",
            "epoch: 2 [3840/60000 (6%)]\t training loss: 0.098132\n",
            "epoch: 2 [4160/60000 (7%)]\t training loss: 0.021622\n",
            "epoch: 2 [4480/60000 (7%)]\t training loss: 0.020830\n",
            "epoch: 2 [4800/60000 (8%)]\t training loss: 0.017926\n",
            "epoch: 2 [5120/60000 (9%)]\t training loss: 0.185184\n",
            "epoch: 2 [5440/60000 (9%)]\t training loss: 0.149663\n",
            "epoch: 2 [5760/60000 (10%)]\t training loss: 0.089693\n",
            "epoch: 2 [6080/60000 (10%)]\t training loss: 0.070531\n",
            "epoch: 2 [6400/60000 (11%)]\t training loss: 0.034534\n",
            "epoch: 2 [6720/60000 (11%)]\t training loss: 0.049013\n",
            "epoch: 2 [7040/60000 (12%)]\t training loss: 0.012918\n",
            "epoch: 2 [7360/60000 (12%)]\t training loss: 0.101116\n",
            "epoch: 2 [7680/60000 (13%)]\t training loss: 0.016830\n",
            "epoch: 2 [8000/60000 (13%)]\t training loss: 0.235459\n",
            "epoch: 2 [8320/60000 (14%)]\t training loss: 0.088455\n",
            "epoch: 2 [8640/60000 (14%)]\t training loss: 0.020136\n",
            "epoch: 2 [8960/60000 (15%)]\t training loss: 0.030928\n",
            "epoch: 2 [9280/60000 (15%)]\t training loss: 0.002639\n",
            "epoch: 2 [9600/60000 (16%)]\t training loss: 0.062741\n",
            "epoch: 2 [9920/60000 (17%)]\t training loss: 0.003497\n",
            "epoch: 2 [10240/60000 (17%)]\t training loss: 0.007688\n",
            "epoch: 2 [10560/60000 (18%)]\t training loss: 0.001969\n",
            "epoch: 2 [10880/60000 (18%)]\t training loss: 0.087715\n",
            "epoch: 2 [11200/60000 (19%)]\t training loss: 0.021731\n",
            "epoch: 2 [11520/60000 (19%)]\t training loss: 0.017555\n",
            "epoch: 2 [11840/60000 (20%)]\t training loss: 0.104052\n",
            "epoch: 2 [12160/60000 (20%)]\t training loss: 0.068002\n",
            "epoch: 2 [12480/60000 (21%)]\t training loss: 0.053844\n",
            "epoch: 2 [12800/60000 (21%)]\t training loss: 0.006709\n",
            "epoch: 2 [13120/60000 (22%)]\t training loss: 0.190556\n",
            "epoch: 2 [13440/60000 (22%)]\t training loss: 0.033643\n",
            "epoch: 2 [13760/60000 (23%)]\t training loss: 0.115461\n",
            "epoch: 2 [14080/60000 (23%)]\t training loss: 0.042927\n",
            "epoch: 2 [14400/60000 (24%)]\t training loss: 0.034154\n",
            "epoch: 2 [14720/60000 (25%)]\t training loss: 0.007279\n",
            "epoch: 2 [15040/60000 (25%)]\t training loss: 0.002353\n",
            "epoch: 2 [15360/60000 (26%)]\t training loss: 0.073108\n",
            "epoch: 2 [15680/60000 (26%)]\t training loss: 0.039223\n",
            "epoch: 2 [16000/60000 (27%)]\t training loss: 0.090295\n",
            "epoch: 2 [16320/60000 (27%)]\t training loss: 0.156239\n",
            "epoch: 2 [16640/60000 (28%)]\t training loss: 0.007567\n",
            "epoch: 2 [16960/60000 (28%)]\t training loss: 0.370483\n",
            "epoch: 2 [17280/60000 (29%)]\t training loss: 0.052054\n",
            "epoch: 2 [17600/60000 (29%)]\t training loss: 0.082525\n",
            "epoch: 2 [17920/60000 (30%)]\t training loss: 0.091500\n",
            "epoch: 2 [18240/60000 (30%)]\t training loss: 0.081759\n",
            "epoch: 2 [18560/60000 (31%)]\t training loss: 0.002456\n",
            "epoch: 2 [18880/60000 (31%)]\t training loss: 0.086008\n",
            "epoch: 2 [19200/60000 (32%)]\t training loss: 0.111545\n",
            "epoch: 2 [19520/60000 (33%)]\t training loss: 0.012260\n",
            "epoch: 2 [19840/60000 (33%)]\t training loss: 0.061752\n",
            "epoch: 2 [20160/60000 (34%)]\t training loss: 0.014947\n",
            "epoch: 2 [20480/60000 (34%)]\t training loss: 0.107861\n",
            "epoch: 2 [20800/60000 (35%)]\t training loss: 0.018007\n",
            "epoch: 2 [21120/60000 (35%)]\t training loss: 0.071745\n",
            "epoch: 2 [21440/60000 (36%)]\t training loss: 0.047280\n",
            "epoch: 2 [21760/60000 (36%)]\t training loss: 0.011887\n",
            "epoch: 2 [22080/60000 (37%)]\t training loss: 0.018028\n",
            "epoch: 2 [22400/60000 (37%)]\t training loss: 0.002997\n",
            "epoch: 2 [22720/60000 (38%)]\t training loss: 0.007426\n",
            "epoch: 2 [23040/60000 (38%)]\t training loss: 0.076983\n",
            "epoch: 2 [23360/60000 (39%)]\t training loss: 0.119280\n",
            "epoch: 2 [23680/60000 (39%)]\t training loss: 0.178616\n",
            "epoch: 2 [24000/60000 (40%)]\t training loss: 0.024068\n",
            "epoch: 2 [24320/60000 (41%)]\t training loss: 0.024153\n",
            "epoch: 2 [24640/60000 (41%)]\t training loss: 0.096584\n",
            "epoch: 2 [24960/60000 (42%)]\t training loss: 0.044256\n",
            "epoch: 2 [25280/60000 (42%)]\t training loss: 0.080778\n",
            "epoch: 2 [25600/60000 (43%)]\t training loss: 0.094158\n",
            "epoch: 2 [25920/60000 (43%)]\t training loss: 0.012723\n",
            "epoch: 2 [26240/60000 (44%)]\t training loss: 0.001965\n",
            "epoch: 2 [26560/60000 (44%)]\t training loss: 0.008722\n",
            "epoch: 2 [26880/60000 (45%)]\t training loss: 0.049342\n",
            "epoch: 2 [27200/60000 (45%)]\t training loss: 0.437475\n",
            "epoch: 2 [27520/60000 (46%)]\t training loss: 0.093486\n",
            "epoch: 2 [27840/60000 (46%)]\t training loss: 0.005935\n",
            "epoch: 2 [28160/60000 (47%)]\t training loss: 0.035793\n",
            "epoch: 2 [28480/60000 (47%)]\t training loss: 0.070024\n",
            "epoch: 2 [28800/60000 (48%)]\t training loss: 0.008796\n",
            "epoch: 2 [29120/60000 (49%)]\t training loss: 0.005460\n",
            "epoch: 2 [29440/60000 (49%)]\t training loss: 0.075453\n",
            "epoch: 2 [29760/60000 (50%)]\t training loss: 0.082600\n",
            "epoch: 2 [30080/60000 (50%)]\t training loss: 0.025095\n",
            "epoch: 2 [30400/60000 (51%)]\t training loss: 0.008946\n",
            "epoch: 2 [30720/60000 (51%)]\t training loss: 0.017521\n",
            "epoch: 2 [31040/60000 (52%)]\t training loss: 0.144723\n",
            "epoch: 2 [31360/60000 (52%)]\t training loss: 0.040355\n",
            "epoch: 2 [31680/60000 (53%)]\t training loss: 0.009017\n",
            "epoch: 2 [32000/60000 (53%)]\t training loss: 0.003635\n",
            "epoch: 2 [32320/60000 (54%)]\t training loss: 0.007101\n",
            "epoch: 2 [32640/60000 (54%)]\t training loss: 0.021724\n",
            "epoch: 2 [32960/60000 (55%)]\t training loss: 0.000807\n",
            "epoch: 2 [33280/60000 (55%)]\t training loss: 0.173847\n",
            "epoch: 2 [33600/60000 (56%)]\t training loss: 0.010778\n",
            "epoch: 2 [33920/60000 (57%)]\t training loss: 0.013701\n",
            "epoch: 2 [34240/60000 (57%)]\t training loss: 0.399148\n",
            "epoch: 2 [34560/60000 (58%)]\t training loss: 0.005925\n",
            "epoch: 2 [34880/60000 (58%)]\t training loss: 0.115400\n",
            "epoch: 2 [35200/60000 (59%)]\t training loss: 0.012402\n",
            "epoch: 2 [35520/60000 (59%)]\t training loss: 0.039522\n",
            "epoch: 2 [35840/60000 (60%)]\t training loss: 0.003374\n",
            "epoch: 2 [36160/60000 (60%)]\t training loss: 0.032543\n",
            "epoch: 2 [36480/60000 (61%)]\t training loss: 0.080096\n",
            "epoch: 2 [36800/60000 (61%)]\t training loss: 0.002783\n",
            "epoch: 2 [37120/60000 (62%)]\t training loss: 0.456542\n",
            "epoch: 2 [37440/60000 (62%)]\t training loss: 0.004737\n",
            "epoch: 2 [37760/60000 (63%)]\t training loss: 0.076467\n",
            "epoch: 2 [38080/60000 (63%)]\t training loss: 0.019500\n",
            "epoch: 2 [38400/60000 (64%)]\t training loss: 0.035769\n",
            "epoch: 2 [38720/60000 (65%)]\t training loss: 0.074662\n",
            "epoch: 2 [39040/60000 (65%)]\t training loss: 0.034344\n",
            "epoch: 2 [39360/60000 (66%)]\t training loss: 0.004834\n",
            "epoch: 2 [39680/60000 (66%)]\t training loss: 0.155939\n",
            "epoch: 2 [40000/60000 (67%)]\t training loss: 0.013532\n",
            "epoch: 2 [40320/60000 (67%)]\t training loss: 0.274997\n",
            "epoch: 2 [40640/60000 (68%)]\t training loss: 0.598912\n",
            "epoch: 2 [40960/60000 (68%)]\t training loss: 0.069245\n",
            "epoch: 2 [41280/60000 (69%)]\t training loss: 0.055428\n",
            "epoch: 2 [41600/60000 (69%)]\t training loss: 0.175866\n",
            "epoch: 2 [41920/60000 (70%)]\t training loss: 0.005689\n",
            "epoch: 2 [42240/60000 (70%)]\t training loss: 0.010322\n",
            "epoch: 2 [42560/60000 (71%)]\t training loss: 0.049259\n",
            "epoch: 2 [42880/60000 (71%)]\t training loss: 0.006694\n",
            "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002140\n",
            "epoch: 2 [43520/60000 (73%)]\t training loss: 0.077839\n",
            "epoch: 2 [43840/60000 (73%)]\t training loss: 0.263577\n",
            "epoch: 2 [44160/60000 (74%)]\t training loss: 0.123324\n",
            "epoch: 2 [44480/60000 (74%)]\t training loss: 0.026996\n",
            "epoch: 2 [44800/60000 (75%)]\t training loss: 0.003256\n",
            "epoch: 2 [45120/60000 (75%)]\t training loss: 0.062740\n",
            "epoch: 2 [45440/60000 (76%)]\t training loss: 0.031622\n",
            "epoch: 2 [45760/60000 (76%)]\t training loss: 0.036767\n",
            "epoch: 2 [46080/60000 (77%)]\t training loss: 0.099838\n",
            "epoch: 2 [46400/60000 (77%)]\t training loss: 0.026845\n",
            "epoch: 2 [46720/60000 (78%)]\t training loss: 0.220059\n",
            "epoch: 2 [47040/60000 (78%)]\t training loss: 0.025425\n",
            "epoch: 2 [47360/60000 (79%)]\t training loss: 0.046500\n",
            "epoch: 2 [47680/60000 (79%)]\t training loss: 0.023531\n",
            "epoch: 2 [48000/60000 (80%)]\t training loss: 0.090858\n",
            "epoch: 2 [48320/60000 (81%)]\t training loss: 0.197350\n",
            "epoch: 2 [48640/60000 (81%)]\t training loss: 0.030297\n",
            "epoch: 2 [48960/60000 (82%)]\t training loss: 0.005242\n",
            "epoch: 2 [49280/60000 (82%)]\t training loss: 0.002309\n",
            "epoch: 2 [49600/60000 (83%)]\t training loss: 0.020609\n",
            "epoch: 2 [49920/60000 (83%)]\t training loss: 0.100927\n",
            "epoch: 2 [50240/60000 (84%)]\t training loss: 0.015600\n",
            "epoch: 2 [50560/60000 (84%)]\t training loss: 0.004015\n",
            "epoch: 2 [50880/60000 (85%)]\t training loss: 0.003906\n",
            "epoch: 2 [51200/60000 (85%)]\t training loss: 0.044343\n",
            "epoch: 2 [51520/60000 (86%)]\t training loss: 0.221729\n",
            "epoch: 2 [51840/60000 (86%)]\t training loss: 0.005866\n",
            "epoch: 2 [52160/60000 (87%)]\t training loss: 0.003225\n",
            "epoch: 2 [52480/60000 (87%)]\t training loss: 0.128596\n",
            "epoch: 2 [52800/60000 (88%)]\t training loss: 0.041003\n",
            "epoch: 2 [53120/60000 (89%)]\t training loss: 0.004447\n",
            "epoch: 2 [53440/60000 (89%)]\t training loss: 0.125383\n",
            "epoch: 2 [53760/60000 (90%)]\t training loss: 0.191748\n",
            "epoch: 2 [54080/60000 (90%)]\t training loss: 0.002825\n",
            "epoch: 2 [54400/60000 (91%)]\t training loss: 0.376830\n",
            "epoch: 2 [54720/60000 (91%)]\t training loss: 0.116594\n",
            "epoch: 2 [55040/60000 (92%)]\t training loss: 0.034064\n",
            "epoch: 2 [55360/60000 (92%)]\t training loss: 0.066723\n",
            "epoch: 2 [55680/60000 (93%)]\t training loss: 0.009449\n",
            "epoch: 2 [56000/60000 (93%)]\t training loss: 0.019677\n",
            "epoch: 2 [56320/60000 (94%)]\t training loss: 0.001398\n",
            "epoch: 2 [56640/60000 (94%)]\t training loss: 0.050175\n",
            "epoch: 2 [56960/60000 (95%)]\t training loss: 0.012732\n",
            "epoch: 2 [57280/60000 (95%)]\t training loss: 0.014090\n",
            "epoch: 2 [57600/60000 (96%)]\t training loss: 0.065312\n",
            "epoch: 2 [57920/60000 (97%)]\t training loss: 0.133847\n",
            "epoch: 2 [58240/60000 (97%)]\t training loss: 0.051631\n",
            "epoch: 2 [58560/60000 (98%)]\t training loss: 0.003867\n",
            "epoch: 2 [58880/60000 (98%)]\t training loss: 0.003583\n",
            "epoch: 2 [59200/60000 (99%)]\t training loss: 0.010058\n",
            "epoch: 2 [59520/60000 (99%)]\t training loss: 0.014423\n",
            "epoch: 2 [59840/60000 (100%)]\t training loss: 0.007865\n",
            "\n",
            "Test dataset: Overall Loss: 0.0406, Overall Accuracy: 9863/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = enumerate(test_dataloader)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "\n",
        "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "qukW4BiwOoBb",
        "outputId": "d4924cb0-0313-413e-dd0e-bbd6ab4ca9d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
        "print(f\"Ground truth is : {sample_targets[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34JAFYYCOrfW",
        "outputId": "d03dae85-e0f1-443f-d14b-0ff4eb477f76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction is : 7\n",
            "Ground truth is : 7\n"
          ]
        }
      ]
    }
  ]
}